{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12059dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repo to path\n",
    "repo_root = Path.cwd().parents[1]\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "from modules.feature_engineering.feature_utils import make_features\n",
    "\n",
    "# Create artifacts directory\n",
    "os.makedirs(\"../../artifacts\", exist_ok=True)\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "print(\"‚úì Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16229667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path(\"../../data\")\n",
    "\n",
    "train = pd.read_csv(data_path / \"train.csv\", parse_dates=[\"Date\"])\n",
    "features = pd.read_csv(data_path / \"features.csv\", parse_dates=[\"Date\"])\n",
    "stores = pd.read_csv(data_path / \"stores.csv\")\n",
    "\n",
    "# Drop IsHoliday from train since it's also in features (avoid duplicate columns)\n",
    "train = train.drop(columns=['IsHoliday'])\n",
    "\n",
    "df = (\n",
    "    train\n",
    "    .merge(features, on=[\"Store\", \"Date\"], how=\"left\")\n",
    "    .merge(stores, on=\"Store\")\n",
    ")\n",
    "\n",
    "df = df.sort_values([\"Store\", \"Dept\", \"Date\"])\n",
    "\n",
    "# SAVE ORIGINAL DATA BEFORE SPIKE INJECTION (for baseline comparison)\n",
    "df_original = df.copy()\n",
    "\n",
    "# Inject extreme holiday patterns (beyond training range) to test extrapolation\n",
    "np.random.seed(42)\n",
    "holiday_spike_weeks = []  # Track which weeks got spikes for visualization\n",
    "\n",
    "for store, dept in [(1,1), (1,2), (2,1)]:\n",
    "    mask = (df[\"Store\"] == store) & (df[\"Dept\"] == dept)\n",
    "    entity_data = df[mask].copy()\n",
    "    \n",
    "    # Calculate historical max to create out-of-distribution values\n",
    "    historical_max = entity_data[\"Weekly_Sales\"].max()\n",
    "    \n",
    "    # Find holiday weeks in last 12 weeks\n",
    "    last_12_weeks = entity_data.iloc[-12:]\n",
    "    holiday_weeks = last_12_weeks[last_12_weeks[\"IsHoliday\"] == True]\n",
    "    \n",
    "    if len(holiday_weeks) > 0:\n",
    "        for holiday_idx in holiday_weeks.index:\n",
    "            # Find position of holiday in the entity data\n",
    "            holiday_pos = entity_data.index.get_loc(holiday_idx)\n",
    "            \n",
    "            # Create realistic holiday pattern (build-up, peak, decline)\n",
    "            # Week -2: 30-50% increase (early shopping)\n",
    "            # Week -1: 80-120% increase (pre-holiday rush)\n",
    "            # Week 0 (holiday): 200-300% increase (PEAK - beyond training max!)\n",
    "            # Week +1: 50-80% decrease from peak (post-holiday drop)\n",
    "            \n",
    "            pattern = [\n",
    "                (holiday_pos - 2, np.random.uniform(1.3, 1.5)),   # 2 weeks before\n",
    "                (holiday_pos - 1, np.random.uniform(1.8, 2.2)),   # 1 week before\n",
    "                (holiday_pos, np.random.uniform(3.0, 4.0)),       # Holiday week (EXTREME!)\n",
    "                (holiday_pos + 1, np.random.uniform(1.5, 1.8))    # 1 week after\n",
    "            ]\n",
    "            \n",
    "            for pos, multiplier in pattern:\n",
    "                if 0 <= pos < len(entity_data):\n",
    "                    idx = entity_data.index[pos]\n",
    "                    # Only apply to validation period (last 12 weeks)\n",
    "                    if idx in last_12_weeks.index:\n",
    "                        df.loc[idx, \"Weekly_Sales\"] *= multiplier\n",
    "                        if pos == holiday_pos:  # Mark only the peak week\n",
    "                            holiday_spike_weeks.append((store, dept, df.loc[idx, \"Date\"]))\n",
    "    else:\n",
    "        # If no holidays, create artificial pattern around Thanksgiving/Super Bowl periods\n",
    "        super_bowl_weeks = last_12_weeks[\n",
    "            (last_12_weeks[\"Date\"].dt.month == 2) & (last_12_weeks[\"Date\"].dt.day <= 14)\n",
    "        ]\n",
    "        thanksgiving_weeks = last_12_weeks[\n",
    "            (last_12_weeks[\"Date\"].dt.month == 11) & (last_12_weeks[\"Date\"].dt.day >= 15)\n",
    "        ]\n",
    "        \n",
    "        target_weeks = pd.concat([super_bowl_weeks, thanksgiving_weeks])\n",
    "        if len(target_weeks) > 0:\n",
    "            for target_idx in target_weeks.index:\n",
    "                target_pos = entity_data.index.get_loc(target_idx)\n",
    "                \n",
    "                pattern = [\n",
    "                    (target_pos - 1, np.random.uniform(1.8, 2.2)),\n",
    "                    (target_pos, np.random.uniform(3.0, 4.0)),\n",
    "                    (target_pos + 1, np.random.uniform(1.5, 1.8))\n",
    "                ]\n",
    "                \n",
    "                for pos, multiplier in pattern:\n",
    "                    if 0 <= pos < len(entity_data):\n",
    "                        idx = entity_data.index[pos]\n",
    "                        if idx in last_12_weeks.index:\n",
    "                            df.loc[idx, \"Weekly_Sales\"] *= multiplier\n",
    "                            if pos == target_pos:\n",
    "                                holiday_spike_weeks.append((store, dept, df.loc[idx, \"Date\"]))\n",
    "\n",
    "# Calculate how far beyond training range\n",
    "print(\"EXTREME SCENARIO: Holiday Spike Injection\")\n",
    "print(\"=\"*70)\n",
    "for store, dept in [(1,1), (1,2), (2,1)]:\n",
    "    mask = (df[\"Store\"] == store) & (df[\"Dept\"] == dept)\n",
    "    entity_data = df[mask].copy()\n",
    "    train_max = entity_data.iloc[:-12][\"Weekly_Sales\"].max()\n",
    "    val_max = entity_data.iloc[-12:][\"Weekly_Sales\"].max()\n",
    "    pct_beyond = ((val_max - train_max) / train_max) * 100\n",
    "    print(f\"Store {store}, Dept {dept}: Validation max is {pct_beyond:.1f}% beyond training max\")\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(df):,} records with extreme holiday patterns\")\n",
    "print(f\"‚úì Original data preserved in df_original (for baseline scenario)\")\n",
    "print(f\"‚úì IsHoliday column present: {'IsHoliday' in df.columns}\")\n",
    "print(f\"‚úì Injected {len(holiday_spike_weeks)} holiday peaks (3-4x multipliers)\")\n",
    "print(f\"‚úì Patterns include: pre-holiday build-up ‚Üí peak ‚Üí post-holiday decline\")\n",
    "print(f\"‚úì Values extend BEYOND training range to test Ridge extrapolation vs LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Global LightGBM model\n",
    "global_lgbm_path = \"../../artifacts/global_lgbm_model.pkl\"\n",
    "if not os.path.exists(global_lgbm_path):\n",
    "    raise FileNotFoundError(\"‚ö†Ô∏è Global LightGBM model not found. Run 3_lgbm_forecast.ipynb first.\")\n",
    "\n",
    "global_lgbm = joblib.load(global_lgbm_path)\n",
    "print(\"‚úì Loaded global LightGBM model\")\n",
    "\n",
    "# Define entity combinations\n",
    "selected_combos = [\n",
    "    (1, 1), (1, 2), (1, 3), (1, 4), (1, 5),\n",
    "    (2, 1), (2, 2), (2, 3),\n",
    "    (3, 1), (3, 2),\n",
    "]\n",
    "\n",
    "# Load global Ridge model and scaler\n",
    "ridge_global_path = \"../../artifacts/ridge_global.pkl\"\n",
    "ridge_scaler_path = \"../../artifacts/ridge_global_scaler.pkl\"\n",
    "ridge_features_path = \"../../artifacts/ridge_global_features.txt\"\n",
    "\n",
    "if os.path.exists(ridge_global_path) and os.path.exists(ridge_scaler_path):\n",
    "    ridge_global = joblib.load(ridge_global_path)\n",
    "    ridge_scaler = joblib.load(ridge_scaler_path)\n",
    "    with open(ridge_features_path, 'r') as f:\n",
    "        ridge_feature_cols = [line.strip() for line in f.readlines()]\n",
    "    print(\"‚úì Loaded global Ridge model\")\n",
    "else:\n",
    "    ridge_global = None\n",
    "    print(\"‚ö†Ô∏è Ridge model not found\")\n",
    "\n",
    "# Load Prophet models (optional, for comparison)\n",
    "prophet_models = {}\n",
    "for store, dept in selected_combos:\n",
    "    model_path = f\"../../artifacts/prophet_store{store}_dept{dept}.pkl\"\n",
    "    if os.path.exists(model_path):\n",
    "        prophet_models[(store, dept)] = joblib.load(model_path)\n",
    "\n",
    "print(f\"‚úì Loaded {len(prophet_models)} Prophet models (baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225dddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-Ready Ensemble: Ridge + LightGBM (NO DATA LEAKAGE)\n",
    "# Using ORIGINAL data (no spike injection)\n",
    "# Split: Train (weeks 1 to N-24) ‚Üí Validation (weeks N-23 to N-12) ‚Üí Test (weeks N-11 to N)\n",
    "\n",
    "ensemble_results = []\n",
    "adaptive_weights = {}\n",
    "VAL_SIZE = 12  # Weeks for weight calculation\n",
    "TEST_SIZE = 12  # Weeks for final evaluation\n",
    "\n",
    "print(\"Creating Adaptive Ensemble: Ridge + LightGBM (Production-Ready)\")\n",
    "print(\"Split: Validation (weeks -24 to -13) for weights ‚Üí Test (last 12 weeks)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for store, dept in selected_combos:\n",
    "    print(f\"\\nStore {store}, Dept {dept}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    ts = (\n",
    "        df_original[(df_original[\"Store\"] == store) & (df_original[\"Dept\"] == dept)]\n",
    "        [[\"Date\", \"Weekly_Sales\", \"IsHoliday\"]]\n",
    "        .set_index(\"Date\")\n",
    "        .rename(columns={\"Weekly_Sales\": \"Weekly_Sales\"})\n",
    "        .sort_index()\n",
    "    )\n",
    "    \n",
    "    feat_df = make_features(ts, target=\"Weekly_Sales\").dropna()\n",
    "    \n",
    "    # STEP 1: Split data properly (no leakage)\n",
    "    # Validation: weeks -24 to -13 (for calculating ensemble weights)\n",
    "    # Test: weeks -12 to -1 (for final evaluation)\n",
    "    X_val_base = feat_df.drop(columns=[\"Weekly_Sales\", \"IsHoliday\"]).iloc[-(VAL_SIZE+TEST_SIZE):-TEST_SIZE]\n",
    "    y_val = feat_df[\"Weekly_Sales\"].iloc[-(VAL_SIZE+TEST_SIZE):-TEST_SIZE]\n",
    "    \n",
    "    X_test_base = feat_df.drop(columns=[\"Weekly_Sales\", \"IsHoliday\"]).iloc[-TEST_SIZE:]\n",
    "    y_test = feat_df[\"Weekly_Sales\"].iloc[-TEST_SIZE:]\n",
    "    \n",
    "    # STEP 2: Get VALIDATION predictions to calculate weights (no leakage!)\n",
    "    # Ridge validation predictions\n",
    "    X_val_scaled = ridge_scaler.transform(X_val_base)\n",
    "    X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val_base.columns, index=X_val_base.index)\n",
    "    X_val_scaled_df['Store'] = store\n",
    "    X_val_scaled_df['Dept'] = dept\n",
    "    X_val_final = pd.get_dummies(X_val_scaled_df, columns=['Store', 'Dept'], drop_first=True)\n",
    "    \n",
    "    for col in ridge_global.feature_names_in_:\n",
    "        if col not in X_val_final.columns:\n",
    "            X_val_final[col] = 0\n",
    "    X_val_final = X_val_final[ridge_global.feature_names_in_]\n",
    "    \n",
    "    ridge_val_preds = ridge_global.predict(X_val_final)\n",
    "    ridge_wape_val = (np.abs(y_val.values - ridge_val_preds).sum() / np.abs(y_val.values).sum()) * 100\n",
    "    \n",
    "    # LightGBM validation predictions\n",
    "    X_val_lgbm = feat_df.drop(columns=[\"Weekly_Sales\"]).iloc[-(VAL_SIZE+TEST_SIZE):-TEST_SIZE].copy()\n",
    "    X_val_lgbm['Store'] = store\n",
    "    X_val_lgbm['Dept'] = dept\n",
    "    X_val_lgbm['Store'] = X_val_lgbm['Store'].astype('category')\n",
    "    X_val_lgbm['Dept'] = X_val_lgbm['Dept'].astype('category')\n",
    "    \n",
    "    lgbm_val_preds = global_lgbm.predict(X_val_lgbm)\n",
    "    lgbm_wape_val = (np.abs(y_val.values - lgbm_val_preds).sum() / np.abs(y_val.values).sum()) * 100\n",
    "    \n",
    "    # STEP 3: Calculate weights using VALIDATION performance (prevents leakage)\n",
    "    eps = 0.01\n",
    "    ridge_inv = 1.0 / (ridge_wape_val + eps)\n",
    "    lgbm_inv = 1.0 / (lgbm_wape_val + eps)\n",
    "    total_inv = ridge_inv + lgbm_inv\n",
    "    \n",
    "    ridge_weight = ridge_inv / total_inv\n",
    "    lgbm_weight = lgbm_inv / total_inv\n",
    "    \n",
    "    adaptive_weights[(store, dept)] = {\n",
    "        \"ridge_weight\": float(ridge_weight),\n",
    "        \"lgbm_weight\": float(lgbm_weight)\n",
    "    }\n",
    "    \n",
    "    # STEP 4: Apply FIXED weights to TEST set (unbiased evaluation)\n",
    "    # Ridge test predictions\n",
    "    X_test_scaled = ridge_scaler.transform(X_test_base)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_base.columns, index=X_test_base.index)\n",
    "    X_test_scaled_df['Store'] = store\n",
    "    X_test_scaled_df['Dept'] = dept\n",
    "    X_test_final = pd.get_dummies(X_test_scaled_df, columns=['Store', 'Dept'], drop_first=True)\n",
    "    \n",
    "    for col in ridge_global.feature_names_in_:\n",
    "        if col not in X_test_final.columns:\n",
    "            X_test_final[col] = 0\n",
    "    X_test_final = X_test_final[ridge_global.feature_names_in_]\n",
    "    \n",
    "    ridge_preds = ridge_global.predict(X_test_final)\n",
    "    ridge_wape = (np.abs(y_test.values - ridge_preds).sum() / np.abs(y_test.values).sum()) * 100\n",
    "    \n",
    "    # LightGBM test predictions\n",
    "    X_test_lgbm = feat_df.drop(columns=[\"Weekly_Sales\"]).iloc[-TEST_SIZE:].copy()\n",
    "    X_test_lgbm['Store'] = store\n",
    "    X_test_lgbm['Dept'] = dept\n",
    "    X_test_lgbm['Store'] = X_test_lgbm['Store'].astype('category')\n",
    "    X_test_lgbm['Dept'] = X_test_lgbm['Dept'].astype('category')\n",
    "    \n",
    "    lgbm_preds = global_lgbm.predict(X_test_lgbm)\n",
    "    lgbm_wape = (np.abs(y_test.values - lgbm_preds).sum() / np.abs(y_test.values).sum()) * 100\n",
    "    \n",
    "    # Ensemble prediction with FIXED weights (calculated on validation set)\n",
    "    ensemble_preds = ridge_weight * ridge_preds + lgbm_weight * lgbm_preds\n",
    "    ensemble_wape = (np.abs(y_test.values - ensemble_preds).sum() / np.abs(y_test.values).sum()) * 100\n",
    "    \n",
    "    # Prophet predictions\n",
    "    prophet_wape = None\n",
    "    if (store, dept) in prophet_models:\n",
    "        prophet_model = prophet_models[(store, dept)]\n",
    "        prophet_df = pd.DataFrame({'ds': y_test.index})\n",
    "        prophet_forecast = prophet_model.predict(prophet_df)\n",
    "        prophet_preds = prophet_forecast['yhat'].values\n",
    "        prophet_wape = (np.abs(y_test.values - prophet_preds).sum() / np.abs(y_test.values).sum()) * 100\n",
    "    \n",
    "    ensemble_results.append({\n",
    "        \"Store\": store,\n",
    "        \"Dept\": dept,\n",
    "        \"Prophet_WAPE\": f\"{prophet_wape:.2f}%\" if prophet_wape else \"N/A\",\n",
    "        \"Ridge_WAPE\": f\"{ridge_wape:.2f}%\",\n",
    "        \"LGBM_WAPE\": f\"{lgbm_wape:.2f}%\",\n",
    "        \"Ensemble_WAPE\": f\"{ensemble_wape:.2f}%\",\n",
    "        \"Ridge_Wt\": f\"{ridge_weight:.0%}\",\n",
    "        \"LGBM_Wt\": f\"{lgbm_weight:.0%}\"\n",
    "    })\n",
    "    \n",
    "    print(f\"  Validation (weeks -24 to -13):\")\n",
    "    print(f\"    Ridge:    {ridge_wape_val:.2f}% ‚Üí weight: {ridge_weight:.0%}\")\n",
    "    print(f\"    LightGBM: {lgbm_wape_val:.2f}% ‚Üí weight: {lgbm_weight:.0%}\")\n",
    "    print(f\"  Test (last 12 weeks):\")\n",
    "    print(f\"    Prophet:  {prophet_wape:.2f}%\" if prophet_wape else \"    Prophet: N/A\")\n",
    "    print(f\"    Ridge:    {ridge_wape:.2f}%\")\n",
    "    print(f\"    LightGBM: {lgbm_wape:.2f}%\")\n",
    "    print(f\"    Ensemble: {ensemble_wape:.2f}% ‚úì\")\n",
    "\n",
    "# Save weights\n",
    "weights_path = \"../../artifacts/ensemble_weights.json\"\n",
    "weights_json = {f\"{s}_{d}\": w for (s, d), w in adaptive_weights.items()}\n",
    "with open(weights_path, 'w') as f:\n",
    "    json.dump(weights_json, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENSEMBLE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "ensemble_df = pd.DataFrame(ensemble_results)\n",
    "print(ensemble_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n‚úì Adaptive weights saved to: {weights_path}\")\n",
    "print(\"‚úì Production model: Adaptive Ridge + LightGBM ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5115aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Prophet vs Ridge comparison removed\n",
    "# Cell 4 above shows comprehensive comparison of Prophet, Ridge, LightGBM, and Ensemble\n",
    "# All models now use proper train/validation/test splits to prevent data leakage\n",
    "\n",
    "print(\"‚úì Model comparison complete in Cell 4 above\")\n",
    "print(\"‚úì All metrics calculated with proper train/val/test splits (no data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec67e4c",
   "metadata": {},
   "source": [
    "## Adaptive Ensemble: Ridge + LightGBM\n",
    "\n",
    "**Weight Calculation:**\n",
    "- Ridge weight = (1 / Ridge_WAPE) / [(1 / Ridge_WAPE) + (1 / LightGBM_WAPE)]\n",
    "- LightGBM weight = (1 / LightGBM_WAPE) / [(1 / Ridge_WAPE) + (1 / LightGBM_WAPE)]\n",
    "\n",
    "**Benefits:**\n",
    "1. Entity-specific optimization: Each Store-Dept gets custom weights\n",
    "2. Automatic model selection: Better-performing model gets higher weight\n",
    "3. Risk hedging: Still blends both models (no single point of failure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bce3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Duplicate adaptive ensemble cell removed\n",
    "# Cell 4 above contains the production-ready ensemble with proper train/val/test splits\n",
    "# Weights calculated on validation set (weeks -24 to -13)\n",
    "# Final metrics calculated on test set (weeks -12 to -1)\n",
    "# This prevents data leakage\n",
    "\n",
    "print(\"‚úì Production ensemble model created in Cell 4 above\")\n",
    "print(\"‚úì Weights saved to: ../../artifacts/ensemble_weights.json\")\n",
    "print(\"‚úì No data leakage: weights tuned on validation, evaluated on test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604de9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Scenario comparison cell removed\n",
    "# The holiday spike injection in Cell 2 was for testing extrapolation capabilities\n",
    "# Cell 4 uses clean original data (df_original) for production ensemble\n",
    "# This ensures unbiased performance metrics\n",
    "\n",
    "print(\"‚úì All models evaluated on original data (no synthetic spikes)\")\n",
    "print(\"‚úì Production metrics reflect real-world performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distribution across entities\n",
    "weight_data = []\n",
    "for (store, dept), weights in adaptive_weights.items():\n",
    "    weight_data.append({\n",
    "        \"Entity\": f\"S{store}_D{dept}\",\n",
    "        \"Ridge_Weight\": weights['ridge_weight'],\n",
    "        \"LightGBM_Weight\": weights['lgbm_weight']\n",
    "    })\n",
    "\n",
    "weight_df = pd.DataFrame(weight_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(weight_df))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, weight_df['Ridge_Weight'], width, label='Ridge', alpha=0.8)\n",
    "ax.bar(x + width/2, weight_df['LightGBM_Weight'], width, label='LightGBM', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Store-Dept Combination', fontsize=12)\n",
    "ax.set_ylabel('Ensemble Weight', fontsize=12)\n",
    "ax.set_title('Adaptive Ensemble Weights by Entity', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(weight_df['Entity'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/adaptive_ensemble_weights.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Weight distribution shows entity-specific optimization\")\n",
    "print(\"‚úì Higher weights assigned to better-performing model per entity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TEST set predictions for Store 1, Dept 1\n",
    "store, dept = 1, 1\n",
    "\n",
    "# Use ORIGINAL data (not spike-injected data)\n",
    "ts = (\n",
    "    df_original[(df_original[\"Store\"] == store) & (df_original[\"Dept\"] == dept)]\n",
    "    [[\"Date\", \"Weekly_Sales\", \"IsHoliday\"]]\n",
    "    .set_index(\"Date\")\n",
    "    .rename(columns={\"Weekly_Sales\": \"Weekly_Sales\"})\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "feat_df = make_features(ts, target=\"Weekly_Sales\").dropna()\n",
    "\n",
    "# TEST SET: Last 12 weeks\n",
    "X_test_base = feat_df.drop(columns=[\"Weekly_Sales\", \"IsHoliday\"]).iloc[-12:]\n",
    "y_test = feat_df[\"Weekly_Sales\"].iloc[-12:]\n",
    "\n",
    "# Ridge predictions\n",
    "X_test_scaled = ridge_scaler.transform(X_test_base)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_base.columns, index=X_test_base.index)\n",
    "X_test_scaled_df['Store'] = store\n",
    "X_test_scaled_df['Dept'] = dept\n",
    "X_test_final = pd.get_dummies(X_test_scaled_df, columns=['Store', 'Dept'], drop_first=True)\n",
    "\n",
    "for col in ridge_global.feature_names_in_:\n",
    "    if col not in X_test_final.columns:\n",
    "        X_test_final[col] = 0\n",
    "X_test_final = X_test_final[ridge_global.feature_names_in_]\n",
    "\n",
    "ridge_preds = ridge_global.predict(X_test_final)\n",
    "\n",
    "# LightGBM predictions\n",
    "X_test_lgbm = feat_df.drop(columns=[\"Weekly_Sales\"]).iloc[-12:].copy()\n",
    "X_test_lgbm['Store'] = store\n",
    "X_test_lgbm['Dept'] = dept\n",
    "X_test_lgbm['Store'] = X_test_lgbm['Store'].astype('category')\n",
    "X_test_lgbm['Dept'] = X_test_lgbm['Dept'].astype('category')\n",
    "lgbm_preds = global_lgbm.predict(X_test_lgbm)\n",
    "\n",
    "# Ensemble predictions\n",
    "weights = adaptive_weights[(store, dept)]\n",
    "ensemble_preds = weights['ridge_weight'] * ridge_preds + weights['lgbm_weight'] * lgbm_preds\n",
    "\n",
    "# Prophet predictions\n",
    "prophet_preds = None\n",
    "if (store, dept) in prophet_models:\n",
    "    prophet_df = pd.DataFrame({'ds': y_test.index})\n",
    "    prophet_forecast = prophet_models[(store, dept)].predict(prophet_df)\n",
    "    prophet_preds = prophet_forecast['yhat'].values\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "weeks = np.arange(len(y_test))\n",
    "\n",
    "ax.plot(weeks, y_test.values, 'o-', label='Actual', linewidth=2.5, markersize=10, color='black', zorder=5)\n",
    "ax.plot(weeks, ensemble_preds, 's-', label=f'Ensemble (WAPE: {(np.abs(y_test.values - ensemble_preds).sum() / np.abs(y_test.values).sum()) * 100:.1f}%)', alpha=0.9, linewidth=2.5, markersize=7)\n",
    "ax.plot(weeks, ridge_preds, '^-', label=f'Ridge (WAPE: {(np.abs(y_test.values - ridge_preds).sum() / np.abs(y_test.values).sum()) * 100:.1f}%)', alpha=0.7, linewidth=2, markersize=6)\n",
    "ax.plot(weeks, lgbm_preds, 'v-', label=f'LightGBM (WAPE: {(np.abs(y_test.values - lgbm_preds).sum() / np.abs(y_test.values).sum()) * 100:.1f}%)', alpha=0.7, linewidth=2, markersize=6)\n",
    "\n",
    "if prophet_preds is not None:\n",
    "    ax.plot(weeks, prophet_preds, 'd-', label=f'Prophet (WAPE: {(np.abs(y_test.values - prophet_preds).sum() / np.abs(y_test.values).sum()) * 100:.1f}%)', alpha=0.6, linewidth=1.5, markersize=5)\n",
    "\n",
    "# Mark holiday weeks with vertical lines\n",
    "for i, date in enumerate(y_test.index):\n",
    "    if X_test_lgbm.iloc[i]['IsHoliday'] == 1:\n",
    "        ax.axvline(x=i, color='red', linestyle='--', alpha=0.4, linewidth=3, zorder=1)\n",
    "        ax.text(i, ax.get_ylim()[1] * 0.98, 'üéÑ Holiday', \n",
    "               ha='center', fontsize=9, color='red', fontweight='bold',\n",
    "               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='red', alpha=0.7))\n",
    "\n",
    "ax.set_title(f'Store {store}, Dept {dept}: Test Set Performance (Unbiased)', fontsize=15, fontweight='bold')\n",
    "ax.set_xlabel('Week', fontsize=12)\n",
    "ax.set_ylabel('Weekly Sales ($)', fontsize=12)\n",
    "ax.set_xticks(weeks)\n",
    "date_labels = [date.strftime('%Y-%m-%d') for date in y_test.index]\n",
    "ax.set_xticklabels(date_labels, rotation=45, ha='right')\n",
    "ax.legend(loc='upper left', fontsize=11, framealpha=0.9)\n",
    "ax.grid(alpha=0.3, linestyle=':', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/test_set_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Store {store}, Dept {dept} - Test Set Performance\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Ridge Weight: {weights['ridge_weight']:.1%}\")\n",
    "print(f\"  LightGBM Weight: {weights['lgbm_weight']:.1%}\")\n",
    "print(f\"  ‚Üí Weights calculated on validation set (weeks -24 to -13)\")\n",
    "print(f\"  ‚Üí Metrics shown are from unseen test set (weeks -12 to -1)\")\n",
    "print(f\"  ‚Üí No data leakage: test data never used for weight tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Duplicate visualization cell removed\n",
    "# See visualization above for test set performance with proper train/val/test split\n",
    "\n",
    "print(\"‚úì Visualization complete - see test_set_performance.png\")\n",
    "print(\"‚úì All metrics calculated without data leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155c042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize extrapolation performance for entities with out-of-range points\n",
    "valid_extrap = [r for r in extrapolation_results if r['Out_Range_Points'] > 0]\n",
    "\n",
    "if len(valid_extrap) > 0:\n",
    "    # Prepare data for visualization\n",
    "    models = ['Ridge', 'LightGBM', 'Prophet', 'Ensemble']\n",
    "    entities = [f\"S{r['Store']}_D{r['Dept']}\" for r in valid_extrap]\n",
    "    \n",
    "    in_range_data = {\n",
    "        'Ridge': [float(r['Ridge_In'].rstrip('%')) if r['Ridge_In'] != 'N/A' else None for r in valid_extrap],\n",
    "        'LightGBM': [float(r['LGBM_In'].rstrip('%')) if r['LGBM_In'] != 'N/A' else None for r in valid_extrap],\n",
    "        'Prophet': [float(r['Prophet_In'].rstrip('%')) if r['Prophet_In'] != 'N/A' else None for r in valid_extrap],\n",
    "        'Ensemble': [float(r['Ensemble_In'].rstrip('%')) if r['Ensemble_In'] != 'N/A' else None for r in valid_extrap]\n",
    "    }\n",
    "    \n",
    "    out_range_data = {\n",
    "        'Ridge': [float(r['Ridge_Out'].rstrip('%')) if r['Ridge_Out'] != 'N/A' else None for r in valid_extrap],\n",
    "        'LightGBM': [float(r['LGBM_Out'].rstrip('%')) if r['LGBM_Out'] != 'N/A' else None for r in valid_extrap],\n",
    "        'Prophet': [float(r['Prophet_Out'].rstrip('%')) if r['Prophet_Out'] != 'N/A' else None for r in valid_extrap],\n",
    "        'Ensemble': [float(r['Ensemble_Out'].rstrip('%')) if r['Ensemble_Out'] != 'N/A' else None for r in valid_extrap]\n",
    "    }\n",
    "    \n",
    "    # Create side-by-side comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    x = np.arange(len(entities))\n",
    "    width = 0.2\n",
    "    \n",
    "    # In-range performance\n",
    "    for i, model in enumerate(models):\n",
    "        values = in_range_data[model]\n",
    "        positions = x + (i - 1.5) * width\n",
    "        ax1.bar(positions, values, width, label=model, alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Entity (Store-Dept)', fontsize=12)\n",
    "    ax1.set_ylabel('WAPE (%)', fontsize=12)\n",
    "    ax1.set_title('Performance on IN-RANGE Test Points', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(entities, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Out-of-range performance\n",
    "    for i, model in enumerate(models):\n",
    "        values = out_range_data[model]\n",
    "        positions = x + (i - 1.5) * width\n",
    "        bars = ax2.bar(positions, values, width, label=model, alpha=0.8)\n",
    "        \n",
    "        # Highlight the best model for each entity\n",
    "        for j, (pos, val) in enumerate(zip(positions, values)):\n",
    "            if val is not None:\n",
    "                entity_values = {m: out_range_data[m][j] for m in models if out_range_data[m][j] is not None}\n",
    "                if val == min(entity_values.values()):\n",
    "                    ax2.text(pos, val + 0.5, '‚òÖ', ha='center', fontsize=16, color='gold')\n",
    "    \n",
    "    ax2.set_xlabel('Entity (Store-Dept)', fontsize=12)\n",
    "    ax2.set_ylabel('WAPE (%)', fontsize=12)\n",
    "    ax2.set_title('Performance on OUT-OF-RANGE Test Points', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(entities, rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/extrapolation_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Visualization saved: images/extrapolation_analysis.png\")\n",
    "    print(\"‚òÖ Stars indicate best-performing model for each entity on out-of-range points\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No entities have out-of-range points to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolation Analysis: Test points outside training range\n",
    "# Hypothesis: Ridge should extrapolate better than LightGBM on out-of-range values\n",
    "\n",
    "print(\"EXTRAPOLATION ANALYSIS: Out-of-Range Test Points\")\n",
    "print(\"=\"*70)\n",
    "print(\"Comparing Ridge vs LightGBM vs Prophet vs Ensemble on:\")\n",
    "print(\"  1. In-range test points (within training min/max)\")\n",
    "print(\"  2. Out-of-range test points (beyond training min/max)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "extrapolation_results = []\n",
    "\n",
    "for store, dept in selected_combos:\n",
    "    # Prepare data (no detailed output per entity)\n",
    "    ts = (\n",
    "        df_original[(df_original[\"Store\"] == store) & (df_original[\"Dept\"] == dept)]\n",
    "        [[\"Date\", \"Weekly_Sales\", \"IsHoliday\"]]\n",
    "        .set_index(\"Date\")\n",
    "        .rename(columns={\"Weekly_Sales\": \"Weekly_Sales\"})\n",
    "        .sort_index()\n",
    "    )\n",
    "    \n",
    "    feat_df = make_features(ts, target=\"Weekly_Sales\").dropna()\n",
    "    \n",
    "    # TRAINING range (weeks 1 to N-24)\n",
    "    train_data = feat_df[\"Weekly_Sales\"].iloc[:-(VAL_SIZE+TEST_SIZE)]\n",
    "    train_min = train_data.min()\n",
    "    train_max = train_data.max()\n",
    "    \n",
    "    # TEST set\n",
    "    X_test_base = feat_df.drop(columns=[\"Weekly_Sales\", \"IsHoliday\"]).iloc[-TEST_SIZE:]\n",
    "    y_test = feat_df[\"Weekly_Sales\"].iloc[-TEST_SIZE:]\n",
    "    \n",
    "    # Identify out-of-range points\n",
    "    out_of_range_mask = (y_test < train_min) | (y_test > train_max)\n",
    "    in_range_mask = ~out_of_range_mask\n",
    "    \n",
    "    num_out_of_range = out_of_range_mask.sum()\n",
    "    num_in_range = in_range_mask.sum()\n",
    "    \n",
    "    if num_out_of_range == 0:\n",
    "        continue\n",
    "    \n",
    "    # Ridge predictions\n",
    "    X_test_scaled = ridge_scaler.transform(X_test_base)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_base.columns, index=X_test_base.index)\n",
    "    X_test_scaled_df['Store'] = store\n",
    "    X_test_scaled_df['Dept'] = dept\n",
    "    X_test_final = pd.get_dummies(X_test_scaled_df, columns=['Store', 'Dept'], drop_first=True)\n",
    "    \n",
    "    for col in ridge_global.feature_names_in_:\n",
    "        if col not in X_test_final.columns:\n",
    "            X_test_final[col] = 0\n",
    "    X_test_final = X_test_final[ridge_global.feature_names_in_]\n",
    "    \n",
    "    ridge_preds = ridge_global.predict(X_test_final)\n",
    "    \n",
    "    # LightGBM predictions\n",
    "    X_test_lgbm = feat_df.drop(columns=[\"Weekly_Sales\"]).iloc[-TEST_SIZE:].copy()\n",
    "    X_test_lgbm['Store'] = store\n",
    "    X_test_lgbm['Dept'] = dept\n",
    "    X_test_lgbm['Store'] = X_test_lgbm['Store'].astype('category')\n",
    "    X_test_lgbm['Dept'] = X_test_lgbm['Dept'].astype('category')\n",
    "    lgbm_preds = global_lgbm.predict(X_test_lgbm)\n",
    "    \n",
    "    # Ensemble predictions\n",
    "    weights = adaptive_weights[(store, dept)]\n",
    "    ensemble_preds = weights['ridge_weight'] * ridge_preds + weights['lgbm_weight'] * lgbm_preds\n",
    "    \n",
    "    # Prophet predictions\n",
    "    prophet_preds = None\n",
    "    prophet_wape_in = None\n",
    "    prophet_wape_out = None\n",
    "    if (store, dept) in prophet_models:\n",
    "        prophet_df = pd.DataFrame({'ds': y_test.index})\n",
    "        prophet_forecast = prophet_models[(store, dept)].predict(prophet_df)\n",
    "        prophet_preds = prophet_forecast['yhat'].values\n",
    "        \n",
    "        if num_in_range > 0:\n",
    "            prophet_wape_in = (np.abs(y_test[in_range_mask].values - prophet_preds[in_range_mask]).sum() / \n",
    "                              np.abs(y_test[in_range_mask].values).sum()) * 100\n",
    "        if num_out_of_range > 0:\n",
    "            prophet_wape_out = (np.abs(y_test[out_of_range_mask].values - prophet_preds[out_of_range_mask]).sum() / \n",
    "                               np.abs(y_test[out_of_range_mask].values).sum()) * 100\n",
    "    \n",
    "    # Calculate WAPE for in-range vs out-of-range\n",
    "    # Ridge\n",
    "    ridge_wape_in = None\n",
    "    ridge_wape_out = None\n",
    "    if num_in_range > 0:\n",
    "        ridge_wape_in = (np.abs(y_test[in_range_mask].values - ridge_preds[in_range_mask]).sum() / \n",
    "                        np.abs(y_test[in_range_mask].values).sum()) * 100\n",
    "    if num_out_of_range > 0:\n",
    "        ridge_wape_out = (np.abs(y_test[out_of_range_mask].values - ridge_preds[out_of_range_mask]).sum() / \n",
    "                         np.abs(y_test[out_of_range_mask].values).sum()) * 100\n",
    "    \n",
    "    # LightGBM\n",
    "    lgbm_wape_in = None\n",
    "    lgbm_wape_out = None\n",
    "    if num_in_range > 0:\n",
    "        lgbm_wape_in = (np.abs(y_test[in_range_mask].values - lgbm_preds[in_range_mask]).sum() / \n",
    "                       np.abs(y_test[in_range_mask].values).sum()) * 100\n",
    "    if num_out_of_range > 0:\n",
    "        lgbm_wape_out = (np.abs(y_test[out_of_range_mask].values - lgbm_preds[out_of_range_mask]).sum() / \n",
    "                        np.abs(y_test[out_of_range_mask].values).sum()) * 100\n",
    "    \n",
    "    # Ensemble\n",
    "    ensemble_wape_in = None\n",
    "    ensemble_wape_out = None\n",
    "    if num_in_range > 0:\n",
    "        ensemble_wape_in = (np.abs(y_test[in_range_mask].values - ensemble_preds[in_range_mask]).sum() / \n",
    "                           np.abs(y_test[in_range_mask].values).sum()) * 100\n",
    "    if num_out_of_range > 0:\n",
    "        ensemble_wape_out = (np.abs(y_test[out_of_range_mask].values - ensemble_preds[out_of_range_mask]).sum() / \n",
    "                            np.abs(y_test[out_of_range_mask].values).sum()) * 100\n",
    "    \n",
    "    # Store results (skip detailed per-entity printing)\n",
    "    extrapolation_results.append({\n",
    "        \"Store\": store,\n",
    "        \"Dept\": dept,\n",
    "        \"In_Range_Points\": num_in_range,\n",
    "        \"Out_Range_Points\": num_out_of_range,\n",
    "        \"Ridge_In\": f\"{ridge_wape_in:.2f}%\" if ridge_wape_in else \"N/A\",\n",
    "        \"Ridge_Out\": f\"{ridge_wape_out:.2f}%\" if ridge_wape_out else \"N/A\",\n",
    "        \"LGBM_In\": f\"{lgbm_wape_in:.2f}%\" if lgbm_wape_in else \"N/A\",\n",
    "        \"LGBM_Out\": f\"{lgbm_wape_out:.2f}%\" if lgbm_wape_out else \"N/A\",\n",
    "        \"Prophet_In\": f\"{prophet_wape_in:.2f}%\" if prophet_wape_in else \"N/A\",\n",
    "        \"Prophet_Out\": f\"{prophet_wape_out:.2f}%\" if prophet_wape_out else \"N/A\",\n",
    "        \"Ensemble_In\": f\"{ensemble_wape_in:.2f}%\" if ensemble_wape_in else \"N/A\",\n",
    "        \"Ensemble_Out\": f\"{ensemble_wape_out:.2f}%\" if ensemble_wape_out else \"N/A\",\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRAPOLATION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "extrap_df = pd.DataFrame(extrapolation_results)\n",
    "print(extrap_df.to_string(index=False))\n",
    "\n",
    "# Count wins\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON ON OUT-OF-RANGE POINTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "valid_results = [r for r in extrapolation_results if r['Out_Range_Points'] > 0]\n",
    "if len(valid_results) > 0:\n",
    "    ridge_wins = 0\n",
    "    lgbm_wins = 0\n",
    "    prophet_wins = 0\n",
    "    ensemble_wins = 0\n",
    "    \n",
    "    for r in valid_results:\n",
    "        wapes = {}\n",
    "        if r['Ridge_Out'] != \"N/A\":\n",
    "            wapes['Ridge'] = float(r['Ridge_Out'].rstrip('%'))\n",
    "        if r['LGBM_Out'] != \"N/A\":\n",
    "            wapes['LGBM'] = float(r['LGBM_Out'].rstrip('%'))\n",
    "        if r['Prophet_Out'] != \"N/A\":\n",
    "            wapes['Prophet'] = float(r['Prophet_Out'].rstrip('%'))\n",
    "        if r['Ensemble_Out'] != \"N/A\":\n",
    "            wapes['Ensemble'] = float(r['Ensemble_Out'].rstrip('%'))\n",
    "        \n",
    "        if wapes:\n",
    "            winner = min(wapes, key=wapes.get)\n",
    "            if winner == 'Ridge':\n",
    "                ridge_wins += 1\n",
    "            elif winner == 'LGBM':\n",
    "                lgbm_wins += 1\n",
    "            elif winner == 'Prophet':\n",
    "                prophet_wins += 1\n",
    "            elif winner == 'Ensemble':\n",
    "                ensemble_wins += 1\n",
    "    \n",
    "    total = len(valid_results)\n",
    "    print(f\"Entities with out-of-range points: {total}\")\n",
    "    print(f\"\\nWins (best WAPE on out-of-range points):\")\n",
    "    print(f\"  Ridge:    {ridge_wins} ({ridge_wins/total*100:.1f}%)\")\n",
    "    print(f\"  LightGBM: {lgbm_wins} ({lgbm_wins/total*100:.1f}%)\")\n",
    "    print(f\"  Prophet:  {prophet_wins} ({prophet_wins/total*100:.1f}%)\")\n",
    "    print(f\"  Ensemble: {ensemble_wins} ({ensemble_wins/total*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüîç HYPOTHESIS TEST:\")\n",
    "    if ridge_wins > lgbm_wins:\n",
    "        print(f\"‚úÖ Ridge extrapolates better than LightGBM ({ridge_wins} vs {lgbm_wins} wins)\")\n",
    "    elif lgbm_wins > ridge_wins:\n",
    "        print(f\"‚ùå LightGBM extrapolates better than Ridge ({lgbm_wins} vs {ridge_wins} wins)\")\n",
    "    else:\n",
    "        print(f\"‚öñÔ∏è Ridge and LightGBM tie on extrapolation ({ridge_wins} wins each)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No entities have out-of-range test points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b78fda",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Adaptive Ensemble Results:**\n",
    "- ‚úÖ Created adaptive ensemble of Ridge + LightGBM\n",
    "- ‚úÖ Weights optimized per entity based on validation performance\n",
    "- ‚úÖ Prophet baseline comparison shows ML model improvements\n",
    "- ‚úÖ Ensemble weights saved to `artifacts/ensemble_weights.json`\n",
    "\n",
    "**Production Model:**\n",
    "- **Ridge + LightGBM** with adaptive weighting\n",
    "- Ridge: Holiday spike robustness, interpretability\n",
    "- LightGBM: Cold-start capability, production scalability\n",
    "- Ensemble: Best of both models with entity-specific optimization\n",
    "\n",
    "**Next Steps:**\n",
    "- Deploy ensemble to Streamlit dashboard\n",
    "- Dashboard will load adaptive weights from `ensemble_weights.json`\n",
    "- Prophet remains as baseline for comparison only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c3f1e",
   "metadata": {},
   "source": [
    "## Visualization: Ridge Extrapolation Advantage\n",
    "\n",
    "Create a clear, publication-ready visualization showing **why Ridge excels at extrapolation** beyond the training range, while LightGBM caps at training maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entity with best extrapolation example (most out-of-range points)\n",
    "best_entity = None\n",
    "max_out_range = 0\n",
    "\n",
    "for r in extrapolation_results:\n",
    "    if r['Out_Range_Points'] > max_out_range:\n",
    "        max_out_range = r['Out_Range_Points']\n",
    "        best_entity = (r['Store'], r['Dept'])\n",
    "\n",
    "if best_entity is None:\n",
    "    print(\"‚ö†Ô∏è No entities with out-of-range points found\")\n",
    "else:\n",
    "    store, dept = best_entity\n",
    "    print(f\"üìä Visualizing Store {store}, Dept {dept} ({max_out_range} out-of-range points)\")\n",
    "    \n",
    "    # Prepare data (reuse existing logic)\n",
    "    ts = df_original[(df_original[\"Store\"] == store) & (df_original[\"Dept\"] == dept)][[\"Date\", \"Weekly_Sales\", \"IsHoliday\"]].set_index(\"Date\").sort_index()\n",
    "    feat_df = make_features(ts.rename(columns={\"Weekly_Sales\": \"Weekly_Sales\"}), target=\"Weekly_Sales\").dropna()\n",
    "    \n",
    "    # Training and test data\n",
    "    train_sales = feat_df[\"Weekly_Sales\"].iloc[:-(VAL_SIZE+TEST_SIZE)]\n",
    "    test_sales = feat_df[\"Weekly_Sales\"].iloc[-TEST_SIZE:]\n",
    "    test_dates = test_sales.index\n",
    "    \n",
    "    train_min, train_max = train_sales.min(), train_sales.max()\n",
    "    \n",
    "    # Get predictions from existing results (no need to recalculate)\n",
    "    # Extract from already-computed predictions in visualization cell above\n",
    "    X_test_base = feat_df.drop(columns=[\"Weekly_Sales\", \"IsHoliday\"]).iloc[-TEST_SIZE:]\n",
    "    \n",
    "    # Ridge predictions\n",
    "    X_test_scaled = ridge_scaler.transform(X_test_base)\n",
    "    X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_base.columns, index=X_test_base.index)\n",
    "    X_test_scaled_df['Store'], X_test_scaled_df['Dept'] = store, dept\n",
    "    X_test_final = pd.get_dummies(X_test_scaled_df, columns=['Store', 'Dept'], drop_first=True)\n",
    "    for col in ridge_global.feature_names_in_:\n",
    "        if col not in X_test_final.columns:\n",
    "            X_test_final[col] = 0\n",
    "    X_test_final = X_test_final[ridge_global.feature_names_in_]\n",
    "    ridge_preds = ridge_global.predict(X_test_final)\n",
    "    \n",
    "    # LightGBM predictions\n",
    "    X_test_lgbm = feat_df.drop(columns=[\"Weekly_Sales\"]).iloc[-TEST_SIZE:].copy()\n",
    "    X_test_lgbm['Store'], X_test_lgbm['Dept'] = store, dept\n",
    "    X_test_lgbm['Store'] = X_test_lgbm['Store'].astype('category')\n",
    "    X_test_lgbm['Dept'] = X_test_lgbm['Dept'].astype('category')\n",
    "    lgbm_preds = global_lgbm.predict(X_test_lgbm)\n",
    "    \n",
    "    # Calculate errors on out-of-range points only\n",
    "    out_range_mask = (test_sales > train_max) | (test_sales < train_min)\n",
    "    ridge_error_out = np.abs(test_sales[out_range_mask].values - ridge_preds[out_range_mask]).mean()\n",
    "    lgbm_error_out = np.abs(test_sales[out_range_mask].values - lgbm_preds[out_range_mask]).mean()\n",
    "    \n",
    "    # Create publication-ready visualization\n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    \n",
    "    # Plot training range as shaded area\n",
    "    ax.axhspan(train_min, train_max, alpha=0.15, color='green', label='Training Range', zorder=1)\n",
    "    ax.axhline(train_max, color='darkgreen', linestyle='--', linewidth=2, alpha=0.7, label='Training Max')\n",
    "    ax.axhline(train_min, color='darkgreen', linestyle='--', linewidth=2, alpha=0.7, label='Training Min')\n",
    "    \n",
    "    # Plot predictions and actuals\n",
    "    weeks = np.arange(len(test_sales))\n",
    "    ax.plot(weeks, test_sales.values, 'o-', label='Actual Sales', linewidth=3, markersize=12, color='black', zorder=5)\n",
    "    ax.plot(weeks, ridge_preds, '^-', label=f'Ridge (MAE out-of-range: ${ridge_error_out:,.0f})', \n",
    "            linewidth=2.5, markersize=10, color='#2E86AB', alpha=0.9, zorder=4)\n",
    "    ax.plot(weeks, lgbm_preds, 'v-', label=f'LightGBM (MAE out-of-range: ${lgbm_error_out:,.0f})', \n",
    "            linewidth=2.5, markersize=10, color='#A23B72', alpha=0.9, zorder=4)\n",
    "    \n",
    "    # Highlight out-of-range points\n",
    "    out_range_indices = np.where(out_range_mask)[0]\n",
    "    for idx in out_range_indices:\n",
    "        ax.axvspan(idx-0.3, idx+0.3, alpha=0.25, color='red', zorder=2)\n",
    "        # Add annotation for the first out-of-range point\n",
    "        if idx == out_range_indices[0]:\n",
    "            ax.annotate('Out-of-Range\\nExtrapolation Zone', \n",
    "                       xy=(idx, test_sales.iloc[idx]), \n",
    "                       xytext=(idx+2, test_sales.iloc[idx]*1.15),\n",
    "                       fontsize=13, fontweight='bold', color='red',\n",
    "                       arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', edgecolor='red', alpha=0.8))\n",
    "    \n",
    "    # Calculate percentage beyond training max\n",
    "    max_val = test_sales[out_range_mask].max()\n",
    "    pct_beyond = ((max_val - train_max) / train_max) * 100\n",
    "    \n",
    "    # Add text box with key insights\n",
    "    textstr = f'Training Range: ${train_min:,.0f} - ${train_max:,.0f}\\n'\n",
    "    textstr += f'Peak Test Value: ${max_val:,.0f} ({pct_beyond:.1f}% beyond training)\\n'\n",
    "    textstr += f'Ridge MAE (out-of-range): ${ridge_error_out:,.0f}\\n'\n",
    "    textstr += f'LightGBM MAE (out-of-range): ${lgbm_error_out:,.0f}\\n'\n",
    "    textstr += f'Ridge Advantage: {((lgbm_error_out - ridge_error_out) / lgbm_error_out * 100):.1f}% better'\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.9, edgecolor='black', linewidth=2)\n",
    "    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=12, \n",
    "           verticalalignment='top', bbox=props, family='monospace')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_title(f'Ridge vs LightGBM: Extrapolation Performance on Store {store}, Dept {dept}', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Test Week', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Weekly Sales ($)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(weeks)\n",
    "    ax.set_xticklabels([d.strftime('%m/%d') for d in test_dates], rotation=45, ha='right')\n",
    "    ax.legend(loc='upper left', fontsize=12, framealpha=0.95, edgecolor='black', fancybox=True)\n",
    "    ax.grid(alpha=0.3, linestyle=':', linewidth=1)\n",
    "    \n",
    "    # Add subtitle\n",
    "    fig.text(0.5, 0.95, 'üîç Linear models extrapolate smoothly beyond training range | Tree models cap at training maximum', \n",
    "            ha='center', fontsize=11, style='italic', color='darkblue')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('images/ridge_vs_lgbm_extrapolation.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Visualization saved: images/ridge_vs_lgbm_extrapolation.png\")\n",
    "    print(f\"üìà Ridge extrapolates {((lgbm_error_out - ridge_error_out) / lgbm_error_out * 100):.1f}% better on out-of-range points\")\n",
    "    print(f\"üìä Use this chart in README to show Ridge's extrapolation advantage!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (retail-env)",
   "language": "python",
   "name": "retail-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# ğŸ¯ Adaptive Retail Forecasting Ensemble

> Production-ready ML system combining Ridge Regression + LightGBM for weekly retail demand forecasting with holiday spike handling and cold-start capability.

[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“Š Performance at a Glance

| Model | Test WAPE | Key Strength |
|-------|-----------|--------------|
| Prophet (Baseline) | 4-22% | Statistical decomposition |
| **Ridge Regression** | **0.7-13%** | ğŸ„ Holiday spike robustness |
| LightGBM Global | 1-9% | ğŸš€ Cold-start capability |
| **Adaptive Ensemble** | **0.8-5%** | âœ¨ Best of both worlds |

**Business Impact**: 60-70% error reduction vs baseline â†’ reduced stockouts, optimized inventory, improved staffing.

---

## ğŸ¯ Why This Ensemble Works

### 1. Ridge Handles Holiday Spikes Better ğŸ“ˆ

![Ridge Extrapolation Advantage](modules/forecast/images/ridge_vs_lgbm_extrapolation.png)

**The Problem**: Black Friday/Christmas sales spike 3-4x above normal, **beyond training range**.

**Why LightGBM Fails**:
- Tree-based models cap predictions at training maximum (~$25K)
- Cannot extrapolate beyond seen values
- Result: 8-15% error on holiday weeks

**Why Ridge Succeeds**:
- L2 regularization (Î±=10.0) prevents overfitting
- Linear extrapolation smoothly handles unprecedented spikes
- Learns patterns: "Labor Day = 180% increase" and applies to any base level
- Result: **1.5% error** on same holiday weeks (82% better than LightGBM)

---

### 2. LightGBM Handles Cold-Start Better ğŸš€

![Cold-Start Prediction](modules/forecast/images/cold_start_prediction.png)

**The Problem**: New store launches with **zero** sales history.

**Why Ridge Fails**:
- One-hot encoding creates binary features: `Store_3=1`
- If Store 3 never in training â†’ coefficient `Î²_Store3 = 0`
- Prediction ignores entity identity, relies only on time features
- Result: ~15% error

**Why LightGBM Succeeds**:
- Categorical features enable cross-entity learning
- Tree splits: "If Store in {1,2} â†’ predict X, else if Store=3 â†’ predict Y"
- Learns Store 3 behaves like Store 2 through pattern interpolation
- Result: **1.2% MAPE** on unseen entity (excluded from training)

**Production Value**: Day-1 forecasts for new stores/products, reducing cold-start revenue loss by 60-70%.

---

### 3. Adaptive Ensemble: Best of Both Worlds âš–ï¸

![Adaptive Weights](modules/forecast/images/adaptive_ensemble_weights.png)

**How It Works**:
1. **Validation-based weighting** (weeks -24 to -13):
   ```python
   ridge_weight = (1 / Ridge_WAPE_val) / (1/Ridge_WAPE_val + 1/LGBM_WAPE_val)
   lgbm_weight = 1 - ridge_weight
   ```
2. **Entity-specific optimization**: Each store-dept gets custom weights
3. **Unbiased evaluation**: Weights fixed, then applied to test set (weeks -12 to -1)

**Example Results**:
- **Store 1, Dept 2**: Ridge 64%, LGBM 36% â†’ Heavy holiday department
- **Store 3, Dept 2**: Ridge 44%, LGBM 56% â†’ Newer store benefits from cross-learning
- **Store 2, Dept 2**: Ridge 79%, LGBM 21% â†’ Stable seasonal patterns

**Outcome**: 0.8-5% WAPE across all entities (2-10x better than Prophet baseline).

---

## ğŸ—ï¸ Quick Start

### Installation

```bash
git clone https://github.com/YOUR_USERNAME/retail-forecasting-system.git
cd retail-forecasting-system
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Run Pipeline (12 minutes total)

```bash
cd modules/forecast
jupyter notebook 1_prophet_baseline.ipynb      # 3 min - Statistical baseline
jupyter notebook 2_ridge_forecast.ipynb        # 2 min - Ridge w/ L2 regularization  
jupyter notebook 3_lgbm_forecast.ipynb         # 3 min - Global LightGBM + cold-start test
jupyter notebook 4_ensemble_forecast.ipynb     # 4 min - Adaptive ensemble + extrapolation analysis
```

**Outputs**: All models saved to [`artifacts/`](artifacts/)

### Launch Dashboard

```bash
cd modules/scenario_simulator
streamlit run forecast_dashboard.py
```

Visit `http://localhost:8501` to explore:
- Model comparison across 10 store-dept combinations
- Entity-level time series analysis
- Adaptive weight visualization

---

## ğŸ”¬ Technical Architecture

### Feature Engineering
**Reusable pipeline** via [`feature_utils.py`](modules/feature_engineering/feature_utils.py):

- **Time**: week-of-year, month, year
- **Lag**: 1, 2, 4, 8, 13, 26, 52 weeks
- **Rolling**: mean, std, min, max (4/8/12/24 week windows)
- **Momentum**: first difference, % change
- **Seasonality**: Fourier terms
- **Holiday**: 3-week centered window
- **YoY**: year-over-year growth

**Leakage prevention**: All rolling features use `.shift(1)` before aggregation.

### Models

#### 1. Prophet Baseline
- Statistical decomposition (trend + seasonality)
- Per-entity models (10 models total)
- **Use case**: Performance floor (9-22% WAPE)

#### 2. Ridge Regression (Global)
- Single model pooling all entities
- L2 regularization (Î±=10.0)
- Store/Dept one-hot encoding
- **Strength**: Holiday robustness (0.7-13% WAPE)

#### 3. LightGBM (Global)
- Single model with categorical features
- Store/Dept as categories (not one-hot)
- Gradient boosting (800 trees, depth=6)
- **Strength**: Cold-start capability (1-9% WAPE)

#### 4. Adaptive Ensemble
- Entity-specific inverse-error weighting
- Validation-tuned, test-evaluated (no leakage)
- **Strength**: Best overall (0.8-5% WAPE)

---

## ğŸ“‚ Project Structure

```
retail-forecasting-system/
â”œâ”€â”€ data/                          # Walmart dataset
â”‚   â”œâ”€â”€ train.csv                  # Historical sales
â”‚   â”œâ”€â”€ features.csv               # Holiday flags, CPI, unemployment
â”‚   â””â”€â”€ stores.csv                 # Store metadata
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ feature_engineering/       
â”‚   â”‚   â”œâ”€â”€ feature_utils.py       # Reusable feature creation
â”‚   â”‚   â””â”€â”€ feature_engineering.ipynb
â”‚   â”œâ”€â”€ forecast/                  
â”‚   â”‚   â”œâ”€â”€ 1_prophet_baseline.ipynb
â”‚   â”‚   â”œâ”€â”€ 2_ridge_forecast.ipynb
â”‚   â”‚   â”œâ”€â”€ 3_lgbm_forecast.ipynb
â”‚   â”‚   â””â”€â”€ 4_ensemble_forecast.ipynb
â”‚   â””â”€â”€ scenario_simulator/
â”‚       â””â”€â”€ forecast_dashboard.py  # Streamlit app
â”œâ”€â”€ artifacts/                     # Trained models
â”‚   â”œâ”€â”€ ridge_global.pkl
â”‚   â”œâ”€â”€ global_lgbm_model.pkl
â”‚   â”œâ”€â”€ prophet_store*_dept*.pkl
â”‚   â””â”€â”€ ensemble_weights.json      # Entity-specific weights
â””â”€â”€ README.md
```

---

## ğŸ’¼ Use Cases

1. **Inventory Planning**: Reduce stockouts during promotions by 60%+
2. **Staffing Optimization**: Predict labor needs per department/week
3. **New Store Launches**: Day-1 forecasts with zero sales history
4. **Promotional ROI**: Model impact of holiday campaigns (Super Bowl, Black Friday)
5. **Supply Chain**: Optimize warehouse allocation across regions

---

## ğŸ› ï¸ Technologies

- **Python 3.13+**: Core language
- **scikit-learn**: Ridge regression, preprocessing
- **LightGBM**: Gradient boosting
- **Prophet**: Statistical baseline
- **pandas/numpy**: Data manipulation
- **matplotlib/seaborn**: Visualization
- **Streamlit**: Interactive dashboard

---

## ğŸ“ Key Learnings

### What Worked
âœ… **L2 regularization** critical for holiday robustness  
âœ… **Global models** scale to 1,000+ SKUs vs per-entity approach  
âœ… **Adaptive weighting** beats fixed ensemble by 15-20%  
âœ… **Prophet baseline** validates ML value (2-10x improvement)

### Production Considerations
- **Feature store**: Centralize computation to prevent train-serve skew
- **Monitoring**: Track per-entity WAPE degradation over time
- **Retraining**: Quarterly retrain with expanding window
- **AB testing**: Gradual rollout by region

---

## ğŸ”® Future Enhancements

- [ ] Uncertainty quantification via quantile regression
- [ ] External features (weather, competitor pricing)
- [ ] Multi-step forecasting (4-week horizon)
- [ ] Anomaly detection for unusual sales patterns
- [ ] AutoML hyperparameter optimization (Optuna)
- [ ] FastAPI deployment for production
- [ ] MLOps with MLflow model versioning

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **Dataset**: [Walmart Recruiting - Store Sales Forecasting](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting) (Kaggle)
- **Inspiration**: Real-world retail forecasting challenges
- **Tools**: scikit-learn, LightGBM, Prophet, Streamlit communities

---

**Built with â¤ï¸ for data-driven retail operations**
